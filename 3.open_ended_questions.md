# Open ended questions

### 1. Primary metric chosen

- Mean absolute percentage error

### 2. Alternative metrics that can be used

- R Squared
- Root Mean Squared Error

### 3. Deciding metrics to use

I have used 3 metrics during evaluation <br>

1. Mean Absolute Percentage Error (MAPE)
   - Measures the percentage of the difference relative to the true value
   - Sensitive to relative error
   - In the case where the magnitude of difference is small, but if the true value is small also, then the error might be significant. This will be taken into account by MAPE
   - It lets us easier to gauge the model performance compared to the true value, as we can expect how many % of error we might have on average when we use it to predict new data
2. R Squared, the coefficient of determination
   - Represents the proportion of variance that can be explained by the independent variables in the model.
   - It provides an indication of goodness of fit and therefore a measure of how well unseen samples are likely to be predicted by the model, through the proportion of explained variance.
3. Root Mean Square Error
   - Gives us the idea of the magnitude of difference between the prediction and true value
   - When we are comparing between 2 models trained with the same dataset, we can easily compare their values of RMSE, to say which model is better
   - However, we do not know how low the error should be in order for us to conclude that the model is good enough.
   - For example, if the true value is 5000, and the predicted value is 4500, RMSE is 500. Wheareas when the true value is 10, and the predicted value is 0, RMSE is 10. But it is still hard for us to conclude that the second case is more desirable.

The primary metric I use is Mean Absolute Percentage Error. This metric is more applicable to real use case, where we usually will set a threshold of acceptable error percentage, and monitor the model's performance. Then, retraining or refitting can be done if the error percentage exceeds the threshold.

### 4. How to find out if my model outperforms the model running on production?

Let the definition of "outperform existing model" to be: New model having a better performance in predicting the most recent periods of COE prices

There are 2 main reasons that new model might outperform existing model:

1. It is being trained with training data that includes most recent data
2. It is being trained with new features, different feature engineering methods or different modelling

So, to make a fair comparison between models, we need to make sure that both models are trained with same period of training data.

Steps:

1. Train an updated version of the existing model by using the same period of train data that was being used to train the new model. But all the other feature engineering and modelling methods must be kept the same as before.
2. Evaluate the updated existing model and the new model on the same test dataset. The metrics are then compared to decide which model is better.
